{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using the matplotlib GUI to input points,\n",
    "# so the following line ensures a non-iPython backend.\n",
    "%matplotlib \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import utils\n",
    "import cv2 as cv\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually defining correspondences \n",
    "\n",
    "The following cells will open the matplotlib GUI for you to select corresponding points on the image. You can run these as many times as you like, if you're not satisfied with the points you selected.\n",
    "\n",
    "Note that the points you select in image 1 and image 2 should:\n",
    "\n",
    "1. Correspond to the same object(s) in the images\n",
    "2. Be selected in the _same order_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# This cell will load and display the first image.\n",
    "# Use the GUI to select four points, which will appear as red crosses on the image.\n",
    "# Close the GUI when you're done.\n",
    "\n",
    "# If this cell gives you an error re: the matplotlib backend, \n",
    "# rerun the first cell.\n",
    "\n",
    "im1 = io.imread('example_images/uttower1.jpg', as_gray=True)\n",
    "plt.imshow(im1, cmap='gray')\n",
    "points1 = plt.ginput(n=4)\n",
    "\n",
    "# If the GUI is giving you trouble,\n",
    "# comment out the above line,\n",
    "# and uncomment the below line.\n",
    "# points1 = np.load('points1.npy')\n",
    "\n",
    "# The points will be stored in array points1\n",
    "# ginput returns points in (x,y), but images are stored in row-major order,\n",
    "# so we flip the coordinates.\n",
    "\n",
    "points1 = np.array([list(p)[::-1] for p in points1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will load and display the second image.\n",
    "# Use the GUI to select four points, which will appear as red crosses on the image.\n",
    "# NOTE: These points should correspond to the points selected in im1.\n",
    "# Close the GUI when you're done.\n",
    "\n",
    "im2 = io.imread('example_images/uttower2.jpg', as_gray=True)\n",
    "plt.imshow(im2, cmap='gray')\n",
    "points2 = plt.ginput(n=4)\n",
    "\n",
    "# If the GUI is giving you trouble,\n",
    "# comment out the above line,\n",
    "# and uncomment the below line.\n",
    "# points2 = np.load('points2.npy')\n",
    "\n",
    "# The points will be stored in array points1\n",
    "# ginput returns points in (x,y), but images are stored in row-major order,\n",
    "# so we flip the coordinates.\n",
    "\n",
    "points2 = np.array([list(p)[::-1] for p in points2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Homographies\n",
    "\n",
    "Fill out the `fit_affine_matrix` function in `panorama.py`, then run the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import fit_affine_matrix\n",
    "\n",
    "# Sanity check for fit_affine_matrix\n",
    "\n",
    "# Test inputs\n",
    "a = np.array([[0.5, 0.1], [0.4, 0.2], [0.8, 0.2]])\n",
    "b = np.array([[0.3, -0.2], [-0.4, -0.9], [0.1, 0.1]])\n",
    "\n",
    "H = fit_affine_matrix(b, a)\n",
    "\n",
    "# Target output\n",
    "sol = np.array(\n",
    "    [[1.25, 2.5, 0.0],\n",
    "     [-5.75, -4.5, 0.0],\n",
    "     [0.25, -1.0, 1.0]]\n",
    ")\n",
    "\n",
    "error = np.sum((H - sol) ** 2)\n",
    "\n",
    "if error < 1e-20:\n",
    "    print('Implementation correct!')\n",
    "else:\n",
    "    print('There is something wrong.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing `fit_affine_matrix`, run the next two cells to warp the images and merge them into a panorama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can switch back to inline matplotlib now.\n",
    "%matplotlib inline\n",
    "\n",
    "from panorama import get_output_space, warp_image\n",
    "\n",
    "\n",
    "# Compute the affine transform between the two images.\n",
    "H = fit_affine_matrix(points1, points2)\n",
    "\n",
    "# get_output_space is a helper function written for you.\n",
    "# It generates a \"canvas\" output space to perform the image warp.\n",
    "\n",
    "output_shape, offset = get_output_space(im1, [im2], [H])\n",
    "print(\"Output shape:\", output_shape)\n",
    "print(\"Offset:\", offset)\n",
    "\n",
    "\n",
    "# Warp images into output sapce\n",
    "img1_warped = warp_image(im1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(im2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "# Plot warped images\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1_warped, cmap='gray')\n",
    "plt.title('Image 1 Warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2_warped, cmap='gray')\n",
    "plt.title('Image 2 Warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two images\n",
    "merged = img1_warped + img2_warped\n",
    "\n",
    "# Track the overlap by adding the masks together\n",
    "overlap = (img1_mask * 1.0 +  # Multiply by 1.0 for bool -> float conversion\n",
    "           img2_mask)\n",
    "\n",
    "# Normalize through division by `overlap` - but ensure the minimum is 1\n",
    "normalized = merged / np.maximum(overlap, 1)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(normalized, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Fit-Affine Panorama - Hand-defined Correspondences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice work! \n",
    "\n",
    "However, as you may have noticed, computing a homography with only four correspondences is unstable and prone to error. Hand-selecting correspondences is not only tedious, it can cause undesirable results if you are even just a few pixels off. To that end, we've provided an implementation of Harris corner detection in order to automatically select keypoints, as well as a method to describe and match the detected keypoints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harris Corner Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('example_images/sudoku.png')\n",
    "gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "gray = np.float32(gray)\n",
    "plt.imshow(gray, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import harris_corners, corner_peaks\n",
    "\n",
    "# Run the function to get Harris corners\n",
    "# response = cv.cornerHarris(gray,3,3,0.04)\n",
    "response = harris_corners(gray)\n",
    "\n",
    "# Run a thresholding function to get the stronger responses\n",
    "corners = corner_peaks(response, threshold_rel=0.01)\n",
    "\n",
    "# Display detected corners\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.scatter(corners[:,1], corners[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Corners')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import harris_corners\n",
    "\n",
    "img1 = cv.imread('example_images/uttower1.jpg')\n",
    "img2 = cv.imread('example_images/uttower2.jpg')\n",
    "\n",
    "img1 = cv.cvtColor(img1,cv.COLOR_BGR2GRAY)\n",
    "img2 = cv.cvtColor(img2,cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect keypoints in two images\n",
    "keypoints1 = corner_peaks(harris_corners(img1, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "keypoints2 = corner_peaks(harris_corners(img2, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "\n",
    "print(\"Keypoints 1 shape = \", keypoints1.shape)\n",
    "print(\"Keypoints 2 shape = \", keypoints2.shape)\n",
    "\n",
    "# Display detected keypoints\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1, cmap='gray')\n",
    "plt.scatter(keypoints1[:,1], keypoints1[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints for Image 1')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2, cmap='gray')\n",
    "plt.scatter(keypoints2[:,1], keypoints2[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints for Image 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import simple_descriptor, match_descriptors, describe_keypoints\n",
    "from utils import plot_matches\n",
    "\n",
    "# Set seed to compare output against solution\n",
    "# np.random.seed(131)\n",
    "\n",
    "patch_size = 5\n",
    "    \n",
    "# Extract features from the corners\n",
    "desc1 = describe_keypoints(img1, keypoints1,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=patch_size)\n",
    "desc2 = describe_keypoints(img2, keypoints2,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=patch_size)\n",
    "\n",
    "print(\"Desc1 shape = \", desc1.shape)\n",
    "print(\"Desc2 shape = \", desc2.shape)\n",
    "\n",
    "# Match descriptors in image1 to those in image2\n",
    "matches = match_descriptors(desc1, desc2, 0.7)\n",
    "\n",
    "# Plot matches\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "ax.axis('off')\n",
    "plt.title('Matched Simple Descriptor')\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, matches)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than directly feeding all our keypoint matches into fit_affine_matrix function, we can instead use RANSAC (\"RANdom SAmple Consensus\") to select only \"inliers\" to use for computing the transformation matrix.\n",
    "\n",
    "The steps of RANSAC are:\n",
    "\n",
    "1. Select random set of matches\n",
    "2. Compute affine transformation matrix\n",
    "3. Find inliers using the given threshold\n",
    "4. Repeat and keep the largest set of inliers\n",
    "5. Re-compute least-squares estimate on all of the inliers. In this case, use Euclidean distance between matched points as a measure of inliers vs outliers.\n",
    "\n",
    "Implement `ransac` in `panorama.py`, run through the following code to get a panorama. You can see the difference from the result we get without RANSAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import ransac\n",
    "\n",
    "# Set seed to compare output against solution image\n",
    "np.random.seed(131)\n",
    "\n",
    "H, robust_matches = ransac(keypoints1, keypoints2, matches,threshold=1)\n",
    "print(\"Robust matches shape = \", robust_matches.shape)\n",
    "print(\"H = \\n\", H)\n",
    "\n",
    "# Visualize robust matches\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, robust_matches)\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Robust Matches')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.imshow(io.imread('solution_ransac.png'))\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Robust Matches Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "\n",
    "# Warp images into output sapce\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Plot warped images\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1_warped, cmap='gray')\n",
    "plt.title('Image 1 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2_warped, cmap='gray')\n",
    "plt.title('Image 2 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge the two images\n",
    "merged = img1_warped + img2_warped\n",
    "\n",
    "# Track the overlap by adding the masks together\n",
    "overlap = (img1_mask * 1.0 +  # Multiply by 1.0 for bool -> float conversion\n",
    "           img2_mask)\n",
    "\n",
    "# Normalize through division by `overlap` - but ensure the minimum is 1\n",
    "normalized = merged / np.maximum(overlap, 1)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(normalized, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Robust Panorama')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(io.imread('solution_ransac_panorama.png'))\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Robust Panorama Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import linear_blend\n",
    "\n",
    "img1 = cv.imread('example_images/uttower1.jpg')\n",
    "img2 = cv.imread('example_images/uttower2.jpg')\n",
    "\n",
    "img1 = cv.cvtColor(img1,cv.COLOR_BGR2GRAY)\n",
    "img2 = cv.cvtColor(img2,cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# Set seed to compare output against solution\n",
    "np.random.seed(131)\n",
    "\n",
    "# Detect keypoints in both images\n",
    "ec1_keypoints1 = corner_peaks(harris_corners(img1, window_size=3),\n",
    "                              threshold_rel=0.05,\n",
    "                              exclude_border=8)\n",
    "ec1_keypoints2 = corner_peaks(harris_corners(img2, window_size=3),\n",
    "                              threshold_rel=0.05,\n",
    "                              exclude_border=8)\n",
    "\n",
    "print(\"EC1 keypoints1 shape = \", ec1_keypoints1.shape)\n",
    "print(\"EC1 keypoints2 shape = \", ec1_keypoints2.shape)\n",
    "\n",
    "# Extract features from the corners\n",
    "ec1_desc1 = describe_keypoints(img1, ec1_keypoints1,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=16)\n",
    "ec1_desc2 = describe_keypoints(img2, ec1_keypoints2,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=16)\n",
    "\n",
    "print(\"EC1 desc1 shape = \", ec1_desc1.shape)\n",
    "print(\"EC1 desc2 shape = \", ec1_desc2.shape)\n",
    "\n",
    "# Match descriptors in image1 to those in image2\n",
    "ec1_matches = match_descriptors(ec1_desc1, ec1_desc2, 0.7)\n",
    "\n",
    "H, robust_matches = ransac(ec1_keypoints1, ec1_keypoints2, ec1_matches, threshold=1)\n",
    "print(\"Robust matches shape = \", robust_matches.shape)\n",
    "print(\"H = \\n\", H)\n",
    "\n",
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "print(\"Output shape:\", output_shape)\n",
    "print(\"Offset:\", offset)\n",
    "\n",
    "# Warp images into output sapce\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Merge the warped images using linear blending scheme\n",
    "merged = linear_blend(img1_warped, img2_warped)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(merged, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Linear Blend')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
